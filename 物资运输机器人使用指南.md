# 物资运输机器人使用指南



## 0 基础知识

### 0.1 环境和设备准备

宇树A1四足机器人一台

RealSense 深度摄像头  D435i 

[Boost](http://www.boost.org) (version 1.5.4 or higher)

[CMake](http://www.cmake.org) (version 2.8.3 or higher)

[LCM](https://lcm-proj.github.io) (version 1.4.0 or higher)

[Socket通信](https://github.com/socketio/socket.io-client/tree/master/dist)

[OpenCV](https://sourceforge.net/projects/opencvlibrary/files/opencv-unix/)

cuDNN 8

CUDA 11.2

TensorRT 8

Xshell 7

XManager 7

场地材料

### 0.2 设备连接

将A1狗开机，调整到站立模式，移动到出发点。电脑连接到A1的WIFI，进行登录。

（1）后板连接

 输入A1狗的密码，即可将电脑连接到A1狗后板

后板包含机器狗的运动相关控制模块，通过接受前板信息，执行预定的机器控制，实现机器狗任务过程中的整体运动及调控。

（2）前板连接

通过终端对前板进行SSH连接 ：`ssh unitree@193.168.123.161`  

输入A1狗的密码，即可将电脑连接到A1狗后板。

前板功能：包含机器狗视觉识别相关文件，负责任务区域及障碍物图像识别处理等相关功能的实现。

通过终端对前板进行SSH连接 ：`ssh unitree@193.168.123.12`  

输入A1狗的密码，即可将电脑连接到A1狗后板。

### 0.3 传输文件

进入文件夹下的目录 `cd robocom2022/examples` 选择控制A1狗的文件 `ls example_walk.cpp`

返回上一层目录 `cd..`  进入build文件夹下  `cd build`

### 0.4  运行程序

对传输完成的文件执行cmake，运行生成的可执行程序。 `make -js`

`cd../build`

`sudo ./example_walk`

在执行此指令之后，机器狗将自动识别，输出对应信息，完成物资运输的任务





### 1.3  基于Yolo及Pytorch的深度学习训练



#### 1.3.0 制作数据集及深度学习模型配置

## 所需环境

torch==1.2.0

yolov5

| YoloV5-V5.0 | https://github.com/bubbliiiing/yolov5-pytorch      |
| :---------- | :------------------------------------------------- |
| YoloV5-V6.1 | https://github.com/bubbliiiing/yolov5-v6.1-pytorch |



## 数据集制作

本次项目我们共制作并使用的数据集标签共1394项，？？数据增强要写吗

#### 正样本的制作：

我们制作的数据集正样本包含四类标签类型：

我们先利用摄像头，利用遥控器操作机器狗走过四类区域，记录走过的视频。利用python脚本将视频切片，得到最原始的图片文件。

对于得到的图片，利用[Make Sense](https://www.makesense.ai/)网站，分别制作成四类标签

打标过程：

（1）登录网站，选择Object detective，确定识别的标签数量和标签名字。	

（2）导入所需标记的图片。

（3）对每张图片识别的区域进行框选，并赋予对应标签。	

（4）完成后从网站下载，得到我们需要的标签压缩文件。

![image-20221102024919715](image-20221102024919715.png)

​																  数据集打标过程

#### 负样本的制作：

由于数据集较小，前期训练识别的时候，会出现识别偏差的情况，为了增强识别的精确度，我们加入了许多负样本。即对于前期采集的图像中，不包含所需要识别标签内容的图片，利用python脚本生成包含对应负样本的标签。

一定要修改路径为对应图片的路径，即可生成对应的标签。

运行该程序，即可生成负样本的对应文件夹

```python

img_path = "D:\dataset\zero_image"			#负样本照片路径
xml_path = "D:\dataset\zero_annotation"		#负样本标注生成路径

```

#### 全样本的图像增强：

由于初始数据集较小，为了提高数据集样本的数量，减少识别误差，提高深度学习效率，我们对此使用了图像增强技术。如下图所示，该好对应的路径，运行图像增强的对应python程序，即可得到增强后的数据集。

```python
    IMG_DIR = "D:\dataset\Image"		#需增强的原图片路径
    XML_DIR = "D:\dataset\Annotation"	#需增强的原图片的标注路径
    
    AUG_XML_DIR = "./AUG/Annotations/"  # 存储增强后的XML文件夹路径 
    AUG_IMG_DIR = "./AUG/JPEGImages/"  # 存储增强后的图片文件夹路径
```



#### 深度学习配置

#### 模型配置：

yolov5包括yolov5_s、yolov5_m、yolov5_l、yolov5_x多种版本的权值文件，本次项目数据量较小，使用的是yolov5_s下的权值配置进行训练。

修改模型配置文件，修改对应的参数权值为yolov5s。

yolo对自定义数据集的配置：
在完成数据集的摆放之后，我们需要利用voc_annotation.py获得训练用的2007_train.txt和2007_val.txt。进入目录后输入以下指令即可运行。

```python
python voc_annotation.py 
```

修改voc_annotation.py里面的参数。

由于是训练自己的数据集，需要在model_data路径下自己建立一个cls_classes.txt文档，文档里面写自己所需要识别区分的标签类别。
本项目的model_data/cls_classes.txt文件内容如下: 倾倒区域、楼梯、障碍、起点/终点

```python
area   	 	
stairs  	
obs     	
begin   	
```

将数据集所有图片标签，存入到VOCdevkit/Voc2007/annotation文件夹。

将数据集所有的图片数据，存入到VOCdevkit/Voc2007/image文件夹。

需要注意的是，图片文件名和标签文件名要一一对应，再自行将所需要文件按照8：1：1的比例划分为训练集、验证集、测试集，写入训练参数路径VOCdevkit/Voc2007/ImageSets/Main下的文档train.txt、val.txt、test.txt，原始yolo文件下若没有该文档就需要新建。

#### 1.3.1 训练数据集

调整参数：对于VOC_annotation文件，将训练参数改为4，加入标签名字，运行voc_annotation.py文件生成对应的train.txt和val.txt，最后使用train.py进行训练。

训练效果 Yolo的过程反馈？？

#### 1.3.2 使用训练的权重进行识别

打开yolo.py文件，在如下部分修改model_path和classes_path使其对应训练好的文件；**model_path对应logs文件夹下面的权值文件，classes_path是model_path对应分的类**。

```python
_defaults = {
    #--------------------------------------------------------------------------#
    #   使用自己训练好的模型进行预测一定要修改model_path和classes_path！
    #   model_path指向logs文件夹下的权值文件，classes_path指向model_data下的txt
    #
    #   训练好后logs文件夹下存在多个权值文件，选择验证集损失较低的即可。
    #   验证集损失较低不代表mAP较高，仅代表该权值在验证集上泛化性能较好。
    #   如果出现shape不匹配，同时要注意训练时的model_path和classes_path参数的修改
    #--------------------------------------------------------------------------#
    "model_path"        : 'model_data/yolov5_s.pth',
    "classes_path"      : 'model_data/coco_classes.txt',
    #---------------------------------------------------------------------#
    #   anchors_path代表先验框对应的txt文件，一般不修改。
    #   anchors_mask用于帮助代码找到对应的先验框，一般不修改。
    #---------------------------------------------------------------------#
    "anchors_path"      : 'model_data/yolo_anchors.txt',
    "anchors_mask"      : [[6, 7, 8], [3, 4, 5], [0, 1, 2]],
    #---------------------------------------------------------------------#
    #   输入图片的大小，必须为32的倍数。
    #---------------------------------------------------------------------#
    "input_shape"       : [640, 640],
    #------------------------------------------------------#
    #   所使用的YoloV5的版本。s、m、l、x
    #------------------------------------------------------#
    "phi"               : 's',
    #---------------------------------------------------------------------#
    #   只有得分大于置信度的预测框会被保留下来
    #---------------------------------------------------------------------#
    "confidence"        : 0.5,
    #---------------------------------------------------------------------#
    #   非极大抑制所用到的nms_iou大小
    #---------------------------------------------------------------------#
    "nms_iou"           : 0.3,
    #---------------------------------------------------------------------#
    #   该变量用于控制是否使用letterbox_image对输入图像进行不失真的resize，
    #   在多次测试后，发现关闭letterbox_image直接resize的效果更好
    #---------------------------------------------------------------------#
    "letterbox_image"   : True,
    #-------------------------------#
    #   是否使用Cuda
    #   没有GPU可以设置成False
    #-------------------------------#
    "cuda"              : True,
}
```

关于识别视频和图像的参数设置，可以参考prdict.py 文件下的相关参数设置

```python
 #-------------------------------------------------------------------------------#
    #   video_path          用于指定视频的路径，当video_path=0时表示检测摄像头
    #                       想要检测视频，则设置如video_path = "xxx.mp4"即可，代表读取出根目录下的xxx.mp4文件。
    #   video_save_path     表示视频保存的路径，当video_save_path=""时表示不保存
    #                       想要保存视频，则设置如video_save_path = "yyy.mp4"即可，代表保存为根目录下的yyy.mp4文件。
    #   video_fps           用于保存的视频的fps
    #
    #   video_path、video_save_path和video_fps仅在mode='video'时有效
    #   保存视频时需要ctrl+c退出或者运行到最后一帧才会完成完整的保存步骤。
    #----------------------------------------------------------------------------#
    video_path      = 0
    video_save_path = ""
    video_fps       = 25.0
    #---------------------------------------------------------------------------#
   

#-------------------------------------------------------------------------#
    #   dir_origin_path     指定了用于检测的图片的文件夹路径
    #   dir_save_path       指定了检测完图片的保存路径
    #   
    #   dir_origin_path和dir_save_path仅在mode='dir_predict'时有效
    #-------------------------------------------------------------------------#
    dir_origin_path = "img/"
    dir_save_path   = "img_out/"
```



### 1.3.4检验图像识别效果展示

运行get_map.py即可获得评估结果，评估结果会保存在map_out文件夹中。？？没有文件夹 没用过？

经过训练调参之后，我们在机器狗任务过程中，对于目标区域的识别效果如下：

![image-20221102024850590](/前板程序/PIC/image-20221102024850590.png)

![image-20221102024858819](/前板程序/PIC/image-20221102024858819.png)

​                             								传递的深度信息效果图

## 1.4 area_detet.py	利用RealSense深度相机 深度学习识别任务区域图像

1.4.1 实现原理：

RealSense实感技术主要采用“主动立体成像原理”，模仿了人眼的“视差原理”。通过相机发出的红外光，以左红外传感器和右红外传感器，分别追踪发出的红外光位置，通过三角定位的原理，计算出3D图像中的“深度”信息。

1.4.2具体实现代码

区域中心点的识别：

```python
def get_mid_pos(frame,result,depth_data):
    distance_list = []
    mid_pos = [(result[1] + result[3])//2, (result[2] + result[4])//2]      #确定索引深度的中心像素位置
    min_val = min(abs(result[3] - result[1]), abs(result[4] - result[2]))   #确定深度搜索范围
    dist = depth_data[int(mid_pos[0]), int(mid_pos[1])]                     #确定
    
    print("curlen")   
    #for i in range(0,480):
    #	print(depth_data[i][320])
    

    #cv2.circle(frame, (int(mid_pos[0]), int(mid_pos[1])), 4, (255,0,0), -1)
    print(int(mid_pos[0] ), int(mid_pos[1]))
    # if dist:
    #     distance_list.append(dist)
    # distance_list = np.array(distance_list)
    # distance_list = np.sort(distance_list)[randnum//2-randnum//4:randnum//2+randnum//4] #冒泡排序+中值滤波
    #print(distance_list, np.mean(distance_list))
    return dist
```



对于目标区域的识别过程：

```python
if __name__ == "__main__":
    clientSocket = socket(AF_INET, SOCK_DGRAM)
    yolo = YOLO()                                                   #调用Yolo()进行区域识别
    pipeline = rs.pipeline()                                        #进行初始化
    config = rs.config()
    #配置深度通道和彩色通道：参数为分辨率、色彩格式，帧率
    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 60)
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 60)

    # Start streaming
    pipeline.start(config)
    try:
        while True:
            # Wait for a coherent pair of frames: depth and color
            frames = pipeline.wait_for_frames()
            depth_frame = frames.get_depth_frame()
            color_frame = frames.get_color_frame()
            if not depth_frame or not color_frame:
                continue
            # Convert images to numpy arrays

            depth_image = np.asanyarray(depth_frame.get_data())     #获取深度图像数据
            color_image = np.asanyarray(color_frame.get_data())     #获取彩色图像数据
            # Apply colormap on depth image (image must be converted to 8-bit per pixel first)
            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)
            # realsense 
            color_image=Image.fromarray(color_image)
            myres = yolo.detect_ourimage(color_image)   #对图像利用yolo进行识别

#如果Yolo识别到目标区域的信息：
            if myres !=None:
                print(myres[0])
                dist=get_mid_pos(color_image, myres, depth_image)/1000
                print(dist)
                #将数据通过Socket传输
                send_bytes = struct.pack('<f',float(dist))+bytes(myres[0],'utf-8')
                clientSocket.sendto(send_bytes, ("192.168.123.161", 12001))
            print("-----------------------")
            color_image=np.array(yolo.detect_image(color_image))
            images = np.hstack((color_image, depth_colormap))
            #Show images
            #cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)      
		   #cv2.imshow('RealSense', color_image)   #彩色图
            key = cv2.waitKey(1)
            # Press esc or 'q' to close the image window
            if 0xFF == ord('q') or key == 27:
                # cv2.destroyAllWindows()
                break
    finally:
        # Stop streaming
        pipeline.stop()
```

识别效果：

![](/前板程序/PIC/image-20221102025101362.png)

​             ![image-20221102025056272](/前板程序/PIC/image-20221102025056272.png)    

## 2.硬件说明

### realsense 相机

其原理是基于三角测量法，左右红外相机进行测量深度，中间红外点阵投射器相当于补光灯，不打开也能测深度，只是效果不好；最右边的RGB相机用于采集彩色图片，最终可以将彩色视频流与深度流进行对齐。通过此方法，可以让计算机更好的感知人类视角下的世界，让人机之间的交互更加自然。鉴于相机是通过发射出的激光散斑编码，获取信息。RealSense发射的是高速变化的编码散斑，因此RealSense输出深度图的帧率，相对于普通的相机，将更加高。例如与Kinect相比，Kinect只有每秒30帧的帧率，RealSense高达每秒100多帧，则使用RealSense可以得到更精准、更实时化的识别效果。

下图是官方对摄像头的原理图示：

![image-20221024201353999](/前板程序/PIC/image-20221024201353999.png)

![image-20221024201432628](/前板程序/PIC/image-20221024201432628.png)
